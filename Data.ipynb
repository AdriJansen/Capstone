{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "## Data\nThe data was collected by the Seattle Police Department and Accident Traffic Records Department from 2004 to present.\nThe data consists of 37 independent variables and 194,673 rows. The dependent variable, \u201cSEVERITYCODE\u201d, contains numbers that correspond to different levels of severity caused by an accident from 0 to 4.\n\nSeverity codes are as follows:\n0: Little to no Probability (Clear Conditions)\n1: Very Low Probability \u2014 Chance or Property Damage\n2: Low Probability \u2014 Chance of Injury\n3: Mild Probability \u2014 Chance of Serious Injury\n4: High Probability \u2014 Chance of Fatality\n\nFurthermore, because of the existence of null values in some records, the data needs to be preprocessed before any further processing.\n\n## Data Preprocessing\nThe dataset in the original form is not ready for data analysis. In order to prepare the data, first, we need to drop the non-relevant columns. In addition, most of the features are of object data types that need to be converted into numerical data types.\n\nAfter analyzing the data set, I have decided to focus on only four features, severity, weather conditions, road conditions, and light conditions, among others.\n\nTo get a good understanding of the dataset, I have checked different values in the features. The results show, the target feature is imbalance, so we use a simple statistical technique to balance it.\n\nThe number of rows in class 1 is almost three times bigger than the number of rows in class 2. It is possible to solve the issue by downsampling the class 1.\n\n## Methodology\nFor implementing the solution, I have used Github as a repository and running Jupyter Notebook to preprocess data and build Machine Learning models. Regarding coding, I have used Python and its popular packages such as Pandas, NumPy and Sklearn.\n\nOnce I have load data into Pandas Dataframe, used \u2018dtypes\u2019 attribute to check the feature names and their data types. Then I have selected the most important features to predict the severity of accidents in Seattle. Among all the features, the following features have the most influence in the accuracy of the predictions:\n\u201cWEATHER\u201d,\n\u201cROADCOND\u201d,\n\u201cLIGHTCOND\u201d\n\nAlso, as I mentioned earlier, \u201cSEVERITYCODE\u201d is the target variable.\n\nI have run a value count on road (\u2018ROADCOND\u2019) and weather condition (\u2018WEATHER\u2019) to get ideas of the different road and weather conditions. I also have run a value count on light condition (\u2019LIGHTCOND\u2019), to see the breakdowns of accidents occurring during the different light conditions. \n\nAfter balancing SEVERITYCODE feature, and standardizing the input feature, the data has been ready for building machine learning models.\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}